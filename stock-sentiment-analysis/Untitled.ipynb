{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd1d32e-f455-4e9e-bd8e-f90338e277bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Roaming\\mamba\\envs\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helpers.clean_data import IndianNewsDataCleaner\n",
    "from helpers.tokenizer_indian import tokenize_function\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from postgres_scripts.read_data import load_financial_news\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0312d432-1309-476f-9670-ddc251048895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pg-finance\n"
     ]
    }
   ],
   "source": [
    "!docker start pg-finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8c2390-5751-4de9-9c3d-e20359956769",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASS')\n",
    "host = os.getenv('DB_HOST')\n",
    "port = os.getenv('DB_PORT')\n",
    "database = os.getenv('DB_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29222b1-ed9d-4f13-9a2a-85d8e4bdbb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_financial_news(database=database, host=host, password=password, port=port, user=user)\n",
    "cleaner = IndianNewsDataCleaner(df, country=\"India\", label='Sentiment')\n",
    "df_clean = (\n",
    "    cleaner\n",
    "    .map_sentiment()\n",
    "    .add_country()\n",
    "    .clean_text()\n",
    "    .filter_data()\n",
    "    .get_clean_data()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd0613f-eabd-41d3-81c5-5bb41f34ff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.moneycontrol.com/news/business/eco...</td>\n",
       "      <td>US consumer spending dropped by a record in Ap...</td>\n",
       "      <td>consumer spending plunges 13.6 percent in Apri...</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.businesstoday.in/top-story/state-r...</td>\n",
       "      <td>State-run lenders require an urgent Rs 1.2 tri...</td>\n",
       "      <td>government will have to take a bulk of the tab...</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.financialexpress.com/economy/covid...</td>\n",
       "      <td>Apparel exporters on Wednesday urged the gover...</td>\n",
       "      <td>exporters are facing issues in terms of raw ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.moneycontrol.com/news/business/mar...</td>\n",
       "      <td>Asian shares battled to extend a global reboun...</td>\n",
       "      <td>the dollar loses some ground on the safe haven...</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.financialexpress.com/industry/six-...</td>\n",
       "      <td>After India’s sovereign credit rating fell to ...</td>\n",
       "      <td>six Indian public-sector undertakings have tak...</td>\n",
       "      <td>0</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL  \\\n",
       "0           0  https://www.moneycontrol.com/news/business/eco...   \n",
       "1           1  https://www.businesstoday.in/top-story/state-r...   \n",
       "2           2  https://www.financialexpress.com/economy/covid...   \n",
       "3           3  https://www.moneycontrol.com/news/business/mar...   \n",
       "4           4  https://www.financialexpress.com/industry/six-...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  US consumer spending dropped by a record in Ap...   \n",
       "1  State-run lenders require an urgent Rs 1.2 tri...   \n",
       "2  Apparel exporters on Wednesday urged the gover...   \n",
       "3  Asian shares battled to extend a global reboun...   \n",
       "4  After India’s sovereign credit rating fell to ...   \n",
       "\n",
       "                                             Summary  Sentiment country  \n",
       "0  consumer spending plunges 13.6 percent in Apri...          0   India  \n",
       "1  government will have to take a bulk of the tab...          0   India  \n",
       "2  exporters are facing issues in terms of raw ma...          0   India  \n",
       "3  the dollar loses some ground on the safe haven...          0   India  \n",
       "4  six Indian public-sector undertakings have tak...          0   India  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb73cf8-a419-414d-82fc-ed53e0f7b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_clean)\n",
    "train_df, test_df = train_test_split(df_clean, test_size=0.2, stratify=df_clean['Sentiment'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['Sentiment'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7f0078f-64bf-46eb-b9e3-53d06970d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[:70])\n",
    "val_dataset = Dataset.from_pandas(val_df[:10])\n",
    "test_dataset = Dataset.from_pandas(test_df[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bbf441e-5433-4f1b-9930-bb9428e8b2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Content': 'Never before in recent history have we had life turned upside down in the manner Covid-19’s danse macabre has done. Never before, in war or calamity, can one recall the cessation of all of the Railways’ 13,500 daily passenger trains and most of its 8,000 freight trains. Bruised like the rest of nation’s economy, Indian Railways (IR) will need to adapt its moribund apparatus to a new paradigm. A kind of state-within-the-state, IR directly provides livelihood to nearly 1 crore of the country’s population. It uniquely touches the life of aam aadmi. By dint of its traditional resilience, amidst the raging pandemic, IR moved record volumes of grains, industrial and consumer goods, large contingents of soldiers to the northern and eastern borders and tens of lakhs of stranded workers, apart from ferrying miscellaneous parcels and inter-city passengers as people resumed their social and economic engagements after the lifting of the lockdown. As pithily put by Donald Rumsfeld, there are known unknowns, and there are unknown unknowns—uncertainties loom large. The post-Covid 19 churn would demand managers to lead and ride the changes. Today, it is not the big that eats the small; it is the fast that eats the slow. IR will perforce need to be alert and agile to adapt, innovate, restructure, and stay close to customers. A seven-point action plan for IR to set its house in order is offered here. Settle IR’s very identity: A rigid bureaucratic structure is antithetical to business ethos. Most railways, including in Russia, China, Germany, etc, are now autonomous corporate entities. IR must shed its role of a departmental undertaking with public service obligation. Re-organise and re-orient freight and passenger businesses: Market needs differ for passenger and freight services; they need to be structured as distinct lines of business. The heart of IR’s freight strategy will be the creation of high-volume, high-speed freight corridors, broadening the focus from captive bulk to competitive traffic, especially non-bulk, calibrating services to create a critical mass of wagons/containers carrying piecemeal general goods, in partnership with other players, for integrated, timely multimodal ‘whole journey’ service. For its passenger business, IR may focus on the core segment of inter-city travel, both sitting services as well as the medium- and long-distance services involving sleeping accommodation, evolving a new genre inter-city rail travel that is substantially expanded, up-graded, and available on demand. It must measure increase in train-speeds not in terms of maximum permissible speed on a route, but in hours and minutes from origin to destination. With the commissioning of the Delhi-Mumbai and Delhi-Kolkata freight corridors by 2021, and the possible bonanza of sectional and terminal capacity further released from drastic curtailment, if not elimination, of short distance “regional” trains, it will be IR’s moment to build an ambitious network of semi high-speed (160-200 km/h) inter-city passenger services on existing conventional tracks. IR must assiduously refrain from short distance (less than 150 km) rail journeys, currently done by a daily average of around 3,800 sectional/“regional”/ordinary train services, which may speedily be corporatised, along with similar corporatisation of suburban train services (around 5,900 trains a day). The sectional/“regional” services, clocking average lead of about 110 km and inflicting heavy losses, apart from eroding route and terminal capacity, make sense neither economically nor ecologically. Transform investment regime, improve project management: Scarce resources have been spread thin on scattered projects. The focus should have been on securing corridor-wise capacity enhancement, tracking of strained routes, last-mile rail linkages, as also passenger terminals and logistics parks and integration of rail with other modes. An endemic problem—time-overruns and cost-overruns —hurts, too. Maximise asset utilisation, restructure ancillary wherewithal: IR will need to maximise resource utilisation and accelerate asset velocity, through proactive maintenance and zero-failure infrastructure and equipment. Besides expeditiously corporatising/privatising its production units as well as all its construction wings, and hiving off its non-core activities and engagements, IR needs to involve reputed institutions and academics for its R&D/IT needs. Rationalise fare and freight charges: By irrationally cross-subsidising passenger services, it has out-priced itself in the freight market. Cut costs, streamline structure: IR has habitually eschewed attempts to control and prune its capital and operating costs, scrutinise and realign offices, workshops, sheds, depots, and yards, even departments, to minimise headcount and create an all-hands-on-deck culture. IR’s budgeted wage bill for FY20, at Rs 86,554 crore, and pensions, at Rs 50,100 crore, account for an unsustainable 66.9% of its working expenses. Contractual payments, at Rs 8,546 crore, represent another 4.16% of working expenses. It will indeed be better served if it first used the secateurs to trim half its gazetted cadres, including over 150 joint secretary-and-above level officers. Restructure and reorganise: The Bibek Debroy Committee found that “IR’s efficiency was better with 9 zones than with 16”. A holistic restructuring is long overdue for IR. It may streamline the traditional 4-tiered organisation into a 3-tiered system, as Chinese Railways did in 2005, by abolishing its 44 sub-regional entities—equivalent of IR’s 68 Divisions. Like the army filling senior ‘command’ posts mostly from the fighting ‘arms’ (infantry and armoured), the posts of the zonal general manager were required to be manned only by officers exposed to rigours of action in the field. IR ignores optimal utilisation of its human capital. As Ugo Bardi argued, “If you want to avoid collapse, you need to embrace change, not fight it”. Moving fast can seem risky, but the bigger risk lies in not changing fast enough. The author is Senior fellow, Asian Institute of Transport Development, Delhi',\n",
       " 'Sentiment': 1,\n",
       " '__index_level_0__': 13680}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4715582-ae3f-48dc-9bf1-0a1841b1bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Running on GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Running on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7b1417-67d0-4bd5-986c-5565d566a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fd6228-463e-4add-8710-6854f5bf9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"model_name\": \"yiyanghkust/finbert-tone\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d319ae-c0eb-4090-8d35-fed9c4e7fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finbert_batch(dataloader, model_dir='./finbert-india'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    pred_probs = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "\n",
    "        true_labels.extend(labels)\n",
    "        pred_labels.extend(preds)\n",
    "        pred_probs.append(probs)\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    pred_probs = np.vstack(pred_probs)\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(true_labels, pred_probs, multi_class='ovo')\n",
    "    except Exception:\n",
    "        auc = None\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f71511-698a-437c-91e7-3a8285c7d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "def evaluate_finbert(test_dataset, model_dir='./finbert-india'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir, use_safetensors=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    pred_probs = []\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        inputs = {k: torch.tensor(v).unsqueeze(0).to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels']\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred = torch.argmax(probs, dim=1).cpu().item()\n",
    "\n",
    "        true_labels.append(labels)\n",
    "        pred_labels.append(pred)\n",
    "        pred_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    pred_probs = np.vstack(pred_probs)\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "    # For multiclass roc_auc, use 'ovo' or 'ovr'\n",
    "    try:\n",
    "        auc = roc_auc_score(true_labels, pred_probs, multi_class='ovo')\n",
    "    except Exception:\n",
    "        auc = None\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    if auc is not None:\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "                yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19962991-8468-45c5-83e9-a7356190504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
    "\n",
    "def train_finbert(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    output_dir='./finbert-india',\n",
    "    model_name='yiyanghkust/finbert-tone',\n",
    "    num_labels=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    logging_steps=500,\n",
    "    seed=42\n",
    "):\n",
    "    # Load model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, use_safetensors=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        save_strategy=save_strategy,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir=logging_dir,\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        metric_for_best_model=metric_for_best_model,\n",
    "        logging_steps=logging_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Training completed. Global step: {train_output.global_step}, Training loss: {train_output.training_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on validation dataset and print metrics\n",
    "    eval_metrics = trainer.evaluate_finbert_batch()\n",
    "    print(\"Evaluation metrics:\", eval_metrics)\n",
    "\n",
    "    # return model and tokenizer if needed for further usage\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52667b9a-8dbf-47f6-a7a1-43cba78cbba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 517\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_datasets(country='India', label='Sentiment', test_size=0.2, val_size=0.1, random_state=42):\n",
    "    # Load env vars\n",
    "    user = os.getenv('DB_USER')\n",
    "    password = os.getenv('DB_PASS')\n",
    "    host = os.getenv('DB_HOST')\n",
    "    port = os.getenv('DB_PORT')\n",
    "    database = os.getenv('DB_NAME')\n",
    "\n",
    "    # Load and clean raw data\n",
    "    df = load_financial_news(database=database, host=host, password=password, port=port, user=user)\n",
    "    cleaner = IndianNewsDataCleaner(df, country=country, label=label)\n",
    "    df_clean = (\n",
    "        cleaner\n",
    "        .map_sentiment()\n",
    "        .add_country()\n",
    "        .clean_text()\n",
    "        .filter_data()\n",
    "        .get_clean_data()\n",
    "    )\n",
    "\n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(df_clean, test_size=test_size, stratify=df_clean[label], random_state=random_state)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=val_size, stratify=train_df[label], random_state=random_state)\n",
    "\n",
    "    # Tokenize\n",
    "    train_dataset = Dataset.from_pandas(train_df[:70])\n",
    "    val_dataset = Dataset.from_pandas(val_df[:10])\n",
    "    test_dataset = Dataset.from_pandas(test_df[:20])\n",
    "\n",
    "    train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    return train_tokenized, val_tokenized, test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76cf3f6e-ea55-4388-a04d-5dd28414f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlflow_experiment(params):\n",
    "    train_dataset, val_dataset, test_dataset = get_tokenized_datasets()\n",
    "    with mlflow.start_run(run_name=\"FinBERT_India\"):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        model, tokenizer, metrics = train_finbert(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "            per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "            num_train_epochs=params[\"num_train_epochs\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            model_name=params[\"model_name\"]\n",
    "        )\n",
    "\n",
    "        mlflow.log_metrics(metrics)\n",
    "        test_metrics = eval_metrics(test_dataset, model_dir=params[\"model_name\"])\n",
    "        mlflow.log_metrics({f\"test_{k}\": v for k, v in test_metrics.items()})\n",
    "        mlflow.transformers.log_model(\n",
    "            transformers_model=model,\n",
    "            artifact_path=\"finbert-india-model\",\n",
    "            tokenizer=tokenizer,\n",
    "            input_example={\"text\": \"The market outlook is positive\"}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1723c81c-13c6-4f5c-92d2-2b360f96721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea63858a-f15a-4734-a284-c366d4ff02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:00<00:00, 71.44 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 57.14 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 68.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "515cb692-bf3c-43dd-99ff-046f3b84eda2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=\u001b[33m\"\u001b[39m\u001b[33mFinBERT_India_TEST\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m     mlflow.log_params(params)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model, tokenizer, metrics = \u001b[43mtrain_finbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mper_device_train_batch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mper_device_eval_batch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_train_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_finbert\u001b[39m\u001b[34m(train_dataset, val_dataset, output_dir, model_name, num_labels, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, num_train_epochs, weight_decay, logging_dir, evaluation_strategy, save_strategy, load_best_model_at_end, metric_for_best_model, logging_steps, seed)\u001b[39m\n\u001b[32m     23\u001b[39m model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, use_safetensors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m tokenizer = BertTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m trainer = Trainer(\n\u001b[32m     43\u001b[39m     model=model,\n\u001b[32m     44\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     48\u001b[39m )\n\u001b[32m     50\u001b[39m trainer.train()\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"FinBERT_India_TEST\"):\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    model, tokenizer, metrics = train_finbert(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        model_name=params[\"model_name\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63847b3-054c-414c-bcc7-219e87c07fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bce665c-62cc-451c-a5bf-f4d6126f6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sande\\appdata\\roaming\\mamba\\envs\\env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02ea77eb-ef10-4aab-992b-651e18fce736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bb131-4cd8-4a0f-b349-412e0bee3541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (micromamba)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
